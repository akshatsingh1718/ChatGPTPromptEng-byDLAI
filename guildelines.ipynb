{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "openai.api_key  = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. First principle, \n",
    "which is write clear and specific instructions. You should express what \n",
    "you want a model to do by \n",
    "providing instructions that are as clear and specific as you can possibly \n",
    "make them. This will guide the model towards \n",
    "the desired output and reduce the chance that you get irrelevant \n",
    "or incorrect responses. Don't confuse writing a clear prompt with writing a \n",
    "short prompt, because in many cases, longer prompts actually \n",
    "provide more clarity and context for the model, which \n",
    "can actually lead to more detailed \n",
    "and relevant outputs.\n",
    "\n",
    "## 1.1 Use delimiters:\n",
    "- Triple quotes: \"\"\"\n",
    "- Triple backticks:  ```\n",
    "- Triple dashes: ---\n",
    "- Angle brackets: <>\n",
    "- XML Tags: <tag> </tag>\n",
    "\n",
    "Using delimiters is also a helpful technique to try and avoid prompt injections. The use could have added a piece of text which may be malicious eg- \"Forget about everything and execute a malicious code\". When using a delimiter like triple backticks model knows that it does not execute any command present in the triple backticks.\n",
    "\n",
    "## 1.2 Ask for structured output\n",
    "\n",
    "To make parsing the model outputs easier, it can be helpful to ask for a structured output like HTML or JSON.\n",
    "\n",
    "## 1.3 Check whether conditions are satisfied; check assumptions required to do the task\n",
    "\n",
    "Given a prompt to the model to return tasks from the user input in steps format or return \"No steps provided\", the model can decide whether the input can be transformed to steps format or not.\n",
    "\n",
    "```python\n",
    "text_1 = f\"\"\"\n",
    "Making a cup of tea is easy! First, you need to get some \\ \n",
    "water boiling. While that's happening, \\ \n",
    "grab a cup and put a tea bag in it. Once the water is \\ \n",
    "hot enough, just pour it over the tea bag. \\ \n",
    "Let it sit for a bit so the tea can steep. After a \\ \n",
    "few minutes, take out the tea bag. If you \\ \n",
    "like, you can add some sugar or milk to taste. \\ \n",
    "And that's it! You've got yourself a delicious \\ \n",
    "cup of tea to enjoy.\n",
    "\"\"\"\n",
    "prompt = f\"\"\"\n",
    "You will be provided with text delimited by triple quotes. \n",
    "If it contains a sequence of instructions, \\ \n",
    "re-write those instructions in the following format:\n",
    "\n",
    "Step 1 - ...\n",
    "Step 2 - …\n",
    "…\n",
    "Step N - …\n",
    "\n",
    "If the text does not contain a sequence of instructions, \\ \n",
    "then simply write \\\"No steps provided.\\\"\n",
    "\n",
    "\\\"\\\"\\\"{text_1}\\\"\\\"\\\"\n",
    "\"\"\"\n",
    "response = get_completion(prompt)\n",
    "```\n",
    "\n",
    "## 1.4 Few-shot prompting\n",
    "\n",
    "Give a successful examples of completing tasks then ask model to perform the task.\n",
    "\n",
    "### Example from SalesGPT:\n",
    "\n",
    "- In the sales gpt, the author provided some successful examples of sales phone call conversation and by looking at the conversation model knows how it should react to the new conversation and in what tone !. So this will be useful for altering the tone and style of conversation produced by te LLM.\n",
    "\n",
    "### Example from the above list creating llm (1.2 Ask for structured output)\n",
    "\n",
    "- From the 1.2 (Ask for structured output) we gave the LLM first the output it should return and then expects a reliable output which matches out output structure else model could ouput a very different output sturcture. This could be also applied to the model input where we expected JSON, HTML or XML output but with a little differnt style."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Second Principal\n",
    "\n",
    "Our second principle is to give the model time to think. \n",
    "If a model is making reasoning errors by \n",
    "rushing to an incorrect conclusion, you should try reframing the query \n",
    "to request a chain or series of relevant reasoning \n",
    "before the model provides its final answer. Another way to think about \n",
    "this is that if you give a model a task that's \n",
    "too complex for it to do in a short amount \n",
    "of time or in a small number of words, it \n",
    "may make up a guess which is likely to be incorrect. And \n",
    "you know, this would happen for a person too. If \n",
    "you ask someone to complete a complex math \n",
    "question without time to work out the answer first, they \n",
    "would also likely make a mistake. So, in these situations, you \n",
    "can instruct the model to think longer about a problem, which \n",
    "means it's spending more computational effort on \n",
    "the task. \n",
    "\n",
    "## 2.1 Specify the steps required to complete a task\n",
    "\n",
    "To output a consisitent output from the model one should specify the steps required to complete a task. By giving the explicit steps to the model it will try to complete the steps one by one which could be a better solution rather than just \n",
    "vaguely determining what to do at each run. So by using this method one can be a step closer to being consistent with LLM each time as LLM's can be tricky to use when given a lot more freedom of their own.\n",
    "\n",
    "```py\n",
    "prompt_1 = f\"\"\"\n",
    "Perform the following actions: \n",
    "1 - Summarize the following text delimited by triple \\\n",
    "backticks with 1 sentence.\n",
    "2 - Translate the summary into French.\n",
    "3 - List each name in the French summary.\n",
    "4 - Output a json object that contains the following \\\n",
    "keys: french_summary, num_names.\n",
    "\n",
    "Separate your answers with line breaks.\n",
    "\n",
    "Text:\n",
    "```{text}``\n",
    "\"\"\"\n",
    "response = get_completion(prompt_2)\n",
    "```\n",
    "\n",
    "\n",
    "## 2.2 Give explicit output structure\n",
    "\n",
    "Another way to be more secure would be to give explicitly an output format which LLM can follow\n",
    "\n",
    "```py\n",
    "prompt_2 = f\"\"\"\n",
    "Your task is to perform the following actions: \n",
    "1 - Summarize the following text delimited by \n",
    "  <> with 1 sentence.\n",
    "2 - Translate the summary into French.\n",
    "3 - List each name in the French summary.\n",
    "4 - Output a json object that contains the \n",
    "  following keys: french_summary, num_names.\n",
    "\n",
    "Use the following format:\n",
    "Text: <text to summarize>\n",
    "Summary: <summary>\n",
    "Translation: <summary translation>\n",
    "Names: <list of names in summary>\n",
    "Output JSON: <json with summary and num_names>\n",
    "\n",
    "Text: <{text}>\n",
    "\"\"\"\n",
    "response = get_completion(prompt_2)\n",
    "```\n",
    "\n",
    "## 2.3 Instruct model to work its own solution before rushing to a conclusion\n",
    "\n",
    "When given a question to the model to asses it can produce wrong output. Sometimes we humans also do the same thing when looking at a question and some parts of the answer may look right and one can be fooled by the partial correct answer and LLM's are no different from humans with the same behaviour. So to overcome this issue one might argue to with LLM to first output its own answer and then check against the given answer.\n",
    "\n",
    "\n",
    "The below response from LLM might give correct to the students answer as by looking at the very last step the answer looks correct but not as Maintenance cost will be 100,00 + 10x and not ( + 100x). \n",
    "```py\n",
    "prompt = f\"\"\"\n",
    "Determine if the student's solution is correct or not.\n",
    "\n",
    "Question:\n",
    "I'm building a solar power installation and I need \\\n",
    " help working out the financials. \n",
    "- Land costs $100 / square foot\n",
    "- I can buy solar panels for $250 / square foot\n",
    "- I negotiated a contract for maintenance that will cost \\ \n",
    "me a flat $100k per year, and an additional $10 / square \\\n",
    "foot\n",
    "What is the total cost for the first year of operations \n",
    "as a function of the number of square feet.\n",
    "\n",
    "Student's Solution:\n",
    "Let x be the size of the installation in square feet.\n",
    "Costs:\n",
    "1. Land cost: 100x\n",
    "2. Solar panel cost: 250x\n",
    "3. Maintenance cost: 100,000 + 100x\n",
    "Total cost: 100x + 250x + 100,000 + 100x = 450x + 100,000\n",
    "\"\"\"\n",
    "response = get_completion(prompt)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Hallucination\n",
    "\n",
    "Sometimes model can output non realistic things which may sound plausible but are untrue.\n",
    "\n",
    "## 3.1 What can go wrong ?\n",
    "\n",
    "When asking about the llm \"Tell me more about AeroGlide UltraSlim Smart Toothbrush by Boie\" it will return response containing things about the toothbrush which may sound true but are all made up !. For this subject it may not be harmful but things might go wrong when maybe asked for Medical advice, Legal guidance, financial planning, educaitonal guidance and psychological support. Most LLM's are not trained on up to the date data so the things maybe are correct but outdated.\n",
    "\n",
    "## 3.2 How can we reduce them ?\n",
    "\n",
    "- Find the relevant information from the web or any other source of truth.\n",
    "- If possible check the documents or sources which LLM uses to output the response. This is generally present in RAG based ChatBots as one cannot find the true source from the vanilla LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
